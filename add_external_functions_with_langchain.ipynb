{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adicionando Funções externad utilizando Langchain\n",
    "\n",
    "Langchain é um framework para criação de aplicações de IA e naturalmente ele possui ferramentas para facilitar a criação de funções externas para serem passadas a api da OpenAI. Para a utilização do LangChain, será necessário entendermos brevemente uma outra biblioteca de Python chamada pydantic, uma biblioteca para validação de dados que facilita a construção de estruturas de dados mais robustas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from typing import Optional\n",
    "from enum import Enum\n",
    "\n",
    "class UnidadeEnum(str, Enum):\n",
    "  celsius = 'celsius'\n",
    "  fahrenheit = 'fahrenheit'\n",
    "  \n",
    "\n",
    "class ObterTemperaturaAtua(BaseModel):\n",
    "  \"\"\"Obtém a temperatura atual de uma determinada localidade\"\"\"\n",
    "  local: str = Field(description='O nome da cidade', examples=['São Paulo', 'Porto Alegre'])\n",
    "  unidade: Optional[UnidadeEnum]\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'ObterTemperaturaAtua',\n",
       " 'description': 'Obtém a temperatura atual de uma determinada localidade',\n",
       " 'parameters': {'type': 'object',\n",
       "  'properties': {'local': {'description': 'O nome da cidade',\n",
       "    'examples': ['São Paulo', 'Porto Alegre'],\n",
       "    'type': 'string'},\n",
       "   'unidade': {'description': 'An enumeration.',\n",
       "    'enum': ['celsius', 'fahrenheit'],\n",
       "    'type': 'string'}},\n",
       "  'required': ['local']}}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.utils.function_calling import convert_to_openai_function\n",
    "\n",
    "tool_temperatura = convert_to_openai_function(ObterTemperaturaAtua)\n",
    "tool_temperatura"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adicionando função externa utilizando LangChain\n",
    "\n",
    "Agora que já sabemos criar funções que os modelos de llm entendam, podemos passar essas funções para os modelos de llm através da biblioteca langchain. Para isso temos duas formas, podemos utilizar o parâmetro functions ao chamar o métdoso chat_models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='', additional_kwargs={'function_call': {'arguments': '{\"local\":\"São Paulo\"}', 'name': 'ObterTemperaturaAtua'}}, response_metadata={'token_usage': {'completion_tokens': 23, 'prompt_tokens': 104, 'total_tokens': 127, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'function_call', 'logprobs': None}, id='run-d8f56aa9-f76a-4d0b-9612-d6bc4b6f3c81-0')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "chat = ChatOpenAI()\n",
    "\n",
    "response = chat.invoke('Qual é a temperatura de Sao Paulo', functions=[tool_temperatura])\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ou podemos dar um bind e criar um novo componente de chat_model que terá acesso a função sempre que for chamado o invoke.\n",
    "Nestes dois casos o modelo se comportará com o parâmetro \"auto\" de cahamento de função ou seja, ele chamará a função quando necessitar, caso contrário se comportará como um modelo de linguagem normal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='', additional_kwargs={'function_call': {'arguments': '{\"local\":\"São Paulo\"}', 'name': 'ObterTemperaturaAtua'}}, response_metadata={'token_usage': {'completion_tokens': 23, 'prompt_tokens': 104, 'total_tokens': 127, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'function_call', 'logprobs': None}, id='run-415f3df0-8b54-4fe7-afe0-9080028c3798-0')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat= ChatOpenAI()\n",
    "\n",
    "chat_com_func = chat.bind(functions=[tool_temperatura])\n",
    "\n",
    "\n",
    "response = chat_com_func.invoke('Qual é a temperatura de Sao Paulo')\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos obrigar o modelo a sempre chamar uma função da seguinte forma:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='', additional_kwargs={'function_call': {'arguments': '{\"local\":\"São Paulo\"}', 'name': 'ObterTemperaturaAtua'}}, response_metadata={'token_usage': {'completion_tokens': 8, 'prompt_tokens': 119, 'total_tokens': 127, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-badbad67-7c95-4ae9-b3bb-062413c047df-0')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "response = chat.invoke(\n",
    "  'Qual é a temperatura de Sao Paulo', \n",
    "  functions=[tool_temperatura],\n",
    "  function_call={'name': 'ObterTemperaturaAtua'}\n",
    "  )\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='', additional_kwargs={'function_call': {'arguments': '{\"local\":\"São Paulo\"}', 'name': 'ObterTemperaturaAtua'}}, response_metadata={'token_usage': {'completion_tokens': 8, 'prompt_tokens': 114, 'total_tokens': 122, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-7049fdeb-1a87-454b-b4ba-93dbf8e2baab-0')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = chat.invoke(\n",
    "  'Olá', \n",
    "  functions=[tool_temperatura],\n",
    "  function_call={'name': 'ObterTemperaturaAtua'}\n",
    "  )\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adicionando a uma chain\n",
    "\n",
    "Podemos adicionar agora este modelo com funções a um prompt e criar uma chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "  ('system', 'Você é um assistente amigável chamado Jeremias'),\n",
    "  ('user', '{input}')\n",
    "])\n",
    "\n",
    "chain = prompt | chat.bind(functions=[tool_temperatura])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Olá! Como posso ajudar você hoje?', response_metadata={'token_usage': {'completion_tokens': 13, 'prompt_tokens': 113, 'total_tokens': 126, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-2907a509-2fb3-4b48-a685-d6b9aec6fa48-0')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({'input': 'Olá'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='', additional_kwargs={'function_call': {'arguments': '{\"local\":\"Itanhaém\"}', 'name': 'ObterTemperaturaAtua'}}, response_metadata={'token_usage': {'completion_tokens': 24, 'prompt_tokens': 119, 'total_tokens': 143, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'function_call', 'logprobs': None}, id='run-edc0ef2b-b907-4f37-9d50-31031d2470a1-0')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({'input': 'Qual a temperatura em Itanhaém'})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
