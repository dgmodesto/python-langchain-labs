{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adicionando Funções externad utilizando Langchain\n",
    "\n",
    "Langchain é um framework para criação de aplicações de IA e naturalmente ele possui ferramentas para facilitar a criação de funções externas para serem passadas a api da OpenAI. Para a utilização do LangChain, será necessário entendermos brevemente uma outra biblioteca de Python chamada pydantic, uma biblioteca para validação de dados que facilita a construção de estruturas de dados mais robustas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from typing import Optional\n",
    "from enum import Enum\n",
    "\n",
    "class UnidadeEnum(str, Enum):\n",
    "  celsius = 'celsius'\n",
    "  fahrenheit = 'fahrenheit'\n",
    "  \n",
    "\n",
    "class ObterTemperaturaAtua(BaseModel):\n",
    "  \"\"\"Obtém a temperatura atual de uma determinada localidade\"\"\"\n",
    "  local: str = Field(description='O nome da cidade', examples=['São Paulo', 'Porto Alegre'])\n",
    "  unidade: Optional[UnidadeEnum]\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'ObterTemperaturaAtua',\n",
       " 'description': 'Obtém a temperatura atual de uma determinada localidade',\n",
       " 'parameters': {'type': 'object',\n",
       "  'properties': {'local': {'description': 'O nome da cidade',\n",
       "    'examples': ['São Paulo', 'Porto Alegre'],\n",
       "    'type': 'string'},\n",
       "   'unidade': {'description': 'An enumeration.',\n",
       "    'enum': ['celsius', 'fahrenheit'],\n",
       "    'type': 'string'}},\n",
       "  'required': ['local']}}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.utils.function_calling import convert_to_openai_function\n",
    "\n",
    "tool_temperatura = convert_to_openai_function(ObterTemperaturaAtua)\n",
    "tool_temperatura"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adicionando função externa utilizando LangChain\n",
    "\n",
    "Agora que já sabemos criar funções que os modelos de llm entendam, podemos passar essas funções para os modelos de llm através da biblioteca langchain. Para isso temos duas formas, podemos utilizar o parâmetro functions ao chamar o métdoso chat_models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='', additional_kwargs={'function_call': {'arguments': '{\"local\":\"São Paulo\"}', 'name': 'ObterTemperaturaAtua'}}, response_metadata={'token_usage': {'completion_tokens': 23, 'prompt_tokens': 104, 'total_tokens': 127, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'function_call', 'logprobs': None}, id='run-d8f56aa9-f76a-4d0b-9612-d6bc4b6f3c81-0')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "chat = ChatOpenAI()\n",
    "\n",
    "response = chat.invoke('Qual é a temperatura de Sao Paulo', functions=[tool_temperatura])\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ou podemos dar um bind e criar um novo componente de chat_model que terá acesso a função sempre que for chamado o invoke.\n",
    "Nestes dois casos o modelo se comportará com o parâmetro \"auto\" de cahamento de função ou seja, ele chamará a função quando necessitar, caso contrário se comportará como um modelo de linguagem normal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='', additional_kwargs={'function_call': {'arguments': '{\"local\":\"São Paulo\"}', 'name': 'ObterTemperaturaAtua'}}, response_metadata={'token_usage': {'completion_tokens': 23, 'prompt_tokens': 104, 'total_tokens': 127, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'function_call', 'logprobs': None}, id='run-415f3df0-8b54-4fe7-afe0-9080028c3798-0')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat= ChatOpenAI()\n",
    "\n",
    "chat_com_func = chat.bind(functions=[tool_temperatura])\n",
    "\n",
    "\n",
    "response = chat_com_func.invoke('Qual é a temperatura de Sao Paulo')\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos obrigar o modelo a sempre chamar uma função da seguinte forma:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='', additional_kwargs={'function_call': {'arguments': '{\"local\":\"São Paulo\"}', 'name': 'ObterTemperaturaAtua'}}, response_metadata={'token_usage': {'completion_tokens': 8, 'prompt_tokens': 119, 'total_tokens': 127, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-badbad67-7c95-4ae9-b3bb-062413c047df-0')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "response = chat.invoke(\n",
    "  'Qual é a temperatura de Sao Paulo', \n",
    "  functions=[tool_temperatura],\n",
    "  function_call={'name': 'ObterTemperaturaAtua'}\n",
    "  )\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='', additional_kwargs={'function_call': {'arguments': '{\"local\":\"São Paulo\"}', 'name': 'ObterTemperaturaAtua'}}, response_metadata={'token_usage': {'completion_tokens': 8, 'prompt_tokens': 114, 'total_tokens': 122, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-7049fdeb-1a87-454b-b4ba-93dbf8e2baab-0')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = chat.invoke(\n",
    "  'Olá', \n",
    "  functions=[tool_temperatura],\n",
    "  function_call={'name': 'ObterTemperaturaAtua'}\n",
    "  )\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adicionando a uma chain\n",
    "\n",
    "Podemos adicionar agora este modelo com funções a um prompt e criar uma chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "  ('system', 'Você é um assistente amigável chamado Jeremias'),\n",
    "  ('user', '{input}')\n",
    "])\n",
    "\n",
    "chain = prompt | chat.bind(functions=[tool_temperatura])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Olá! Como posso ajudar você hoje?', response_metadata={'token_usage': {'completion_tokens': 13, 'prompt_tokens': 113, 'total_tokens': 126, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-2907a509-2fb3-4b48-a685-d6b9aec6fa48-0')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({'input': 'Olá'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='', additional_kwargs={'function_call': {'arguments': '{\"local\":\"Itanhaém\"}', 'name': 'ObterTemperaturaAtua'}}, response_metadata={'token_usage': {'completion_tokens': 24, 'prompt_tokens': 119, 'total_tokens': 143, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'function_call', 'logprobs': None}, id='run-edc0ef2b-b907-4f37-9d50-31031d2470a1-0')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({'input': 'Qual a temperatura em Itanhaém'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Um exemplo mais interessante \n",
    "\n",
    "Indo para um exemplo um pouco mais complexo. Digamos que temos um chatbot e queremos fazer um toreamento para os setores de interesse, no nosso caso atendimento_cliente, dúvidas_alunos, vendas, spam. A primeira etapa é criar um modelo que entenda a solicitação do cliente e direcione para o setor certo. Nas próximas etapas o atendimento pode ser continuado por pessoas ou por agentes especializados para as tarefas do setor. Retiramos as seguintes mensagens de email rececidos pela equipe de atendimento e vamos tentar criar um direcionamento para elas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "duvidas = [\n",
    "  \"Bom dia, gostaria de saber se ha um certificado final para cada trilha ou se os certificados são somente para todos os cursos\",\n",
    "  \"In Esy, Amazon, eBay, Shopify, Pinterest+SEO +II = high sales results\",\n",
    "  \"Boa tarde, esto iniciando hoje  e estou perdido. Tenho vairos objtivos. Não sei nada de programação, exceto que utilizo nos estudos, Podem meu ajudar e me tornar um profissional em programação?\",\n",
    "  \"Bom, dia. Não estou conseguindo tirar os certificados dos cursos que conclui.\",\n",
    "  \"Bom dia. Não encontrei no site o preço de um curso avulso. Saberiam me informar?\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum \n",
    "from langchain.pydantic_v1 import BaseModel, Field "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SetorEnum(str, Enum):\n",
    "  atendimento_cliente = 'atendimento_cliente'\n",
    "  duvidas_aluno = 'duvidas_aluno'\n",
    "  vendas = 'vendas'\n",
    "  spam = 'spam'\n",
    "  \n",
    "class DirecionaSetorResponsavel(BaseModel):\n",
    "  \"\"\"Direciona a dúvida de um cliente ou aluno da escola de programação Asimov para o setor responsável\"\"\"\n",
    "  setor: SetorEnum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.utils.function_calling import convert_to_openai_function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'DirecionaSetorResponsavel',\n",
       " 'description': 'Direciona a dúvida de um cliente ou aluno da escola de programação Asimov para o setor responsável',\n",
       " 'parameters': {'type': 'object',\n",
       "  'properties': {'setor': {'description': 'An enumeration.',\n",
       "    'enum': ['atendimento_cliente', 'duvidas_aluno', 'vendas', 'spam'],\n",
       "    'type': 'string'}},\n",
       "  'required': ['setor']}}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tool_direcionamento = convert_to_openai_function(DirecionaSetorResponsavel)\n",
    "tool_direcionamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.output_parsers.openai_functions import JsonOutputFunctionsParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    ('system', 'Pense com cuidado ao categorizar o texto conforme as instruções'),\n",
    "    ('user', '{input}')\n",
    "])\n",
    "\n",
    "chat = ChatOpenAI()\n",
    "\n",
    "chain = (\n",
    "          prompt \n",
    "          | chat.bind(functions=[tool_direcionamento], function_call={'name': 'DirecionaSetorResponsavel'})\n",
    "          | JsonOutputFunctionsParser()\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dúvida Bom dia. Não encontrei no site o preço de um curso avulso. Saberiam me informar?\n",
      "Resposta {'setor': 'atendimento_cliente'}\n"
     ]
    }
   ],
   "source": [
    "duvida = duvidas[4]\n",
    "resposta = chain.invoke({'input': duvida})\n",
    "\n",
    "print('Dúvida', duvida)\n",
    "print('Resposta', resposta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neste caso gostaríamos que a dúvida fosse direcionada para vendas. Podemos melhorar nosso prompt para aumentar a chance do modelo responder como gostaríamos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = '''Pense com cuidado ao categorizar o texto conforme as instruções.\n",
    "    Questões relacionadas a dúvidas de preço, sobre o produto, como funciona devem ser direcionados para \"vendas\".\n",
    "    Questões relacionadas a conta, acesso a plataforma, a cancelamento e renovação de assinatura devem ser direcionadas para \"atendimento_cliente\".\n",
    "    Questões relacinadas a dúvidas técnicas de programação, conteúdos da plataforma ou tecnologias na área da programação, devem ser direcionadas para \"duvidas_aluno\".\n",
    "    Mensagens suspeitas, em outras linguas que não português, contendo links devem ser direcionadas para \"spam\"\n",
    "     '''\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    ('system', system_message),\n",
    "    ('user', '{input}')\n",
    "])\n",
    "\n",
    "chat = ChatOpenAI()\n",
    "\n",
    "chain = (\n",
    "          prompt \n",
    "          | chat.bind(functions=[tool_direcionamento], function_call={'name': 'DirecionaSetorResponsavel'})\n",
    "          | JsonOutputFunctionsParser()\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dúvida In Esy, Amazon, eBay, Shopify, Pinterest+SEO +II = high sales results\n",
      "Resposta {'setor': 'spam'}\n"
     ]
    }
   ],
   "source": [
    "duvida = duvidas[1]\n",
    "resposta = chain.invoke({'input': duvida})\n",
    "\n",
    "print('Dúvida', duvida)\n",
    "print('Resposta', resposta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
