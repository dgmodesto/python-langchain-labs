{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adicionando funções externas a API da OpenAI\n",
    "\n",
    "Um grande salto de possibilidades de utilizações únicas da LLMs ocorreu quando a OpenAI lançou a function calling. \n",
    "Essa ferramenta permite adicionarmos manualmente funções externas ao modelo que ele, dependendo da situação, poderá utilizar para obter novas informações ou atuar em diversos escopos. Vamos fazer uma breve revisão de como utilizamos funções externas na api da OpenAI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importação Iniciais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "\n",
    "import openai \n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv(find_dotenv())\n",
    "\n",
    "client = openai.Client()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Criando função que será adicionada ao modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obter_temperatura_atual(local, unidade='celsius'):\n",
    "  if 'são paulo' in local.lower():\n",
    "    return json.dumps(\n",
    "      {\"local\": \"Sao Paulo\", \"temperatura\": \"32\", \"unidade\": unidade}\n",
    "    )\n",
    "  elif 'porto alegre' in local.lower():\n",
    "    return json.dumps(\n",
    "      {\"local\": \"Porto Alegre\", \"temperatura\": \"28\", \"unidade\": unidade}\n",
    "    )\n",
    "  else:\n",
    "    return json.dumps(\n",
    "      {\"local\": local, \"temperatura\": \"unknown\"}\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"local\": \"Sao Paulo\", \"temperatura\": \"32\", \"unidade\": \"celsius\"}'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obter_temperatura_atual(\"São Paulo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Criando descrição da função \n",
    "\n",
    "Através dessa descrição o modelo entenderá o que a função faz e como ela pode ser utilizada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [\n",
    "  {\n",
    "    'type': 'function',\n",
    "    'function': {\n",
    "      'name': 'obter_temperatura_atual',\n",
    "      'description': 'Obtém a temperatura atual em uma dada cidade',\n",
    "      'parameters': {\n",
    "        'type': 'object',\n",
    "        'properties': {\n",
    "          'local': {\n",
    "            'type': 'string',\n",
    "            'description': 'O nome da cidade, Ex: São Paulo'\n",
    "          },\n",
    "          'unidade': {\n",
    "            'type': 'string',\n",
    "            'enum': ['celsius', 'fahrenheit']\n",
    "          }\n",
    "        },\n",
    "        'required': ['local'],\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chamando o modelo com a nova ferramenta\n",
    "\n",
    "Para chamar o modelo com a ferramenta criada, basta passar o argumento tools com uma lista de ferramentas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletion(id='chatcmpl-AfHadqyIzPO4YWDUGcpsuijs5hpUG', choices=[Choice(finish_reason='tool_calls', index=0, logprobs=None, message=ChatCompletionMessage(content=None, refusal=None, role='assistant', audio=None, function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_XDUpzU4aUFB59iaNv8UfiT95', function=Function(arguments='{\"local\":\"São Paulo\"}', name='obter_temperatura_atual'), type='function')]))], created=1734402427, model='gpt-3.5-turbo-0125', object='chat.completion', service_tier=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=20, prompt_tokens=86, total_tokens=106, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mensagens = [\n",
    "  { 'role': 'user', 'content': 'Qual é a temperatura em São Paulo agora?' }\n",
    "]\n",
    "\n",
    "resposta = client.chat.completions.create(\n",
    "  model='gpt-3.5-turbo-0125',\n",
    "  messages=mensagens,\n",
    "  tools = tools,\n",
    "  tool_choice='auto'\n",
    ")\n",
    "\n",
    "resposta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analisando a resposta\n",
    "\n",
    "Podemos perceber que o conteúdo da resposta veio vazio, pois para a pergunta \"Qual é a temperatura em São Paulo agora?\" ele necessitará chamar a função anes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletionMessage(content=None, refusal=None, role='assistant', audio=None, function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_XDUpzU4aUFB59iaNv8UfiT95', function=Function(arguments='{\"local\":\"São Paulo\"}', name='obter_temperatura_atual'), type='function')])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mensagem = resposta.choices[0].message\n",
    "mensagem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "mensagem.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ChatCompletionMessageToolCall(id='call_XDUpzU4aUFB59iaNv8UfiT95', function=Function(arguments='{\"local\":\"São Paulo\"}', name='obter_temperatura_atual'), type='function')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mensagem.tool_calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "obter_temperatura_atual\n",
      "{\"local\":\"São Paulo\"}\n"
     ]
    }
   ],
   "source": [
    "tool_call = mensagem.tool_calls[0]\n",
    "print(tool_call.function.name)\n",
    "print(tool_call.function.arguments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adicionando resultado da função as mensages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"local\": \"Sao Paulo\", \"temperatura\": \"32\", \"unidade\": \"celsius\"}'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "observacao = obter_temperatura_atual(**json.loads(tool_call.function.arguments))\n",
    "observacao"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chamando novamente o modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'user', 'content': 'Qual é a temperatura em São Paulo agora?'},\n",
       " ChatCompletionMessage(content=None, refusal=None, role='assistant', audio=None, function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_XDUpzU4aUFB59iaNv8UfiT95', function=Function(arguments='{\"local\":\"São Paulo\"}', name='obter_temperatura_atual'), type='function')])]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mensagens.append(mensagem)\n",
    "mensagens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'user', 'content': 'Qual é a temperatura em São Paulo agora?'},\n",
       " ChatCompletionMessage(content=None, refusal=None, role='assistant', audio=None, function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_XDUpzU4aUFB59iaNv8UfiT95', function=Function(arguments='{\"local\":\"São Paulo\"}', name='obter_temperatura_atual'), type='function')]),\n",
       " {'tool_call_id': 'call_XDUpzU4aUFB59iaNv8UfiT95',\n",
       "  'role': 'tool',\n",
       "  'name': 'obter_temperatura_atual',\n",
       "  'content': '{\"local\": \"Sao Paulo\", \"temperatura\": \"32\", \"unidade\": \"celsius\"}'}]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mensagens.append({\n",
    "  'tool_call_id': tool_call.id,\n",
    "  'role': 'tool',\n",
    "  'name': tool_call.function.name,\n",
    "  'content': observacao\n",
    "})\n",
    "mensagens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletion(id='chatcmpl-AfHn8DZPpIthV39EXO7ZZYg8rp96E', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='A temperatura atual em São Paulo é de 32°C.', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None))], created=1734403202, model='gpt-3.5-turbo-0125', object='chat.completion', service_tier=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=13, prompt_tokens=142, total_tokens=155, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resposta = client.chat.completions.create(\n",
    "  model='gpt-3.5-turbo-0125',\n",
    "  messages=mensagens,\n",
    "  tools=tools,\n",
    "  tool_choice='auto'\n",
    ")\n",
    "resposta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A temperatura atual em São Paulo é de 32°C.'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resposta.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explorando diferentes perguntas e o parâmetro tool_choice\n",
    "\n",
    "Através do parâmetro tool_choice é possível forçar o modelo a sempre utiliar uma tool. Vamos ver como ele se comporta para diferentes perguntas modificando a parâmetro."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parâmetro \"auto\"\n",
    "\n",
    "Assim o modelo define automaticamente se é necessária a utilização de uma função ou não"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Content:  None\n",
      "Tools:  [ChatCompletionMessageToolCall(id='call_g4yynZ04iX3XAxH9zhpNYJ0W', function=Function(arguments='{\"local\":\"São Paulo\"}', name='obter_temperatura_atual'), type='function')]\n"
     ]
    }
   ],
   "source": [
    "mensagens = [\n",
    "  { 'role': 'user', 'content': 'Qual é a temperatura em São Paulo' }\n",
    "]\n",
    "\n",
    "resposta = client.chat.completions.create(\n",
    "  model='gpt-3.5-turbo-0125',\n",
    "  messages=mensagens,\n",
    "  tools=tools,\n",
    "  tool_choice='auto'\n",
    ")\n",
    "mensagem = resposta.choices[0].message\n",
    "print('Content: ', mensagem.content)\n",
    "print('Tools: ', mensagem.tool_calls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Content:  Olá! Estou aqui para te ajudar. Como posso ajudar você hoje?\n",
      "Tools:  None\n"
     ]
    }
   ],
   "source": [
    "mensagens = [\n",
    "  { 'role': 'user', 'content': 'Oi, tudo bem?' }\n",
    "]\n",
    "\n",
    "resposta = client.chat.completions.create(\n",
    "  model='gpt-3.5-turbo-0125',\n",
    "  messages=mensagens,\n",
    "  tools=tools,\n",
    "  tool_choice='auto'\n",
    ")\n",
    "mensagem = resposta.choices[0].message\n",
    "print('Content: ', mensagem.content)\n",
    "print('Tools: ', mensagem.tool_calls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parâmetro \"none\"\n",
    "\n",
    "Com o parâmetro \"none\", o modelo não vai utilizar funções."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Content:  Vou verificar a temperatura atual em São Paulo para você.\n",
      "Tools:  None\n"
     ]
    }
   ],
   "source": [
    "mensagens = [\n",
    "  { 'role': 'user', 'content': 'Qual é a temperatura em São Paulo' }\n",
    "]\n",
    "\n",
    "resposta = client.chat.completions.create(\n",
    "  model='gpt-3.5-turbo-0125',\n",
    "  messages=mensagens,\n",
    "  tools=tools,\n",
    "  tool_choice='none'\n",
    ")\n",
    "mensagem = resposta.choices[0].message\n",
    "print('Content: ', mensagem.content)\n",
    "print('Tools: ', mensagem.tool_calls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Content:  Olá! Como posso te ajudar hoje?\n",
      "Tools:  None\n"
     ]
    }
   ],
   "source": [
    "mensagens = [\n",
    "  { 'role': 'user', 'content': 'Olá' }\n",
    "]\n",
    "\n",
    "resposta = client.chat.completions.create(\n",
    "  model='gpt-3.5-turbo-0125',\n",
    "  messages=mensagens,\n",
    "  tools=tools,\n",
    "  tool_choice='none'\n",
    ")\n",
    "mensagem = resposta.choices[0].message\n",
    "print('Content: ', mensagem.content)\n",
    "print('Tools: ', mensagem.tool_calls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parâmetro \"function\"\n",
    "\n",
    "Podemos fazer o modelo rodar obrigatoriamente a função, passando dentro de um dicionário a função que o modelo deve executar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Content:  None\n",
      "Tools:  [ChatCompletionMessageToolCall(id='call_QYmsaGFYadmR2yTwPbYbsnjz', function=Function(arguments='{\"local\":\"São Paulo\"}', name='obter_temperatura_atual'), type='function')]\n"
     ]
    }
   ],
   "source": [
    "mensagens = [\n",
    "  { 'role': 'user', 'content': 'Qual é a temperatura em São Paulo' }\n",
    "]\n",
    "\n",
    "resposta = client.chat.completions.create(\n",
    "  model='gpt-3.5-turbo-0125',\n",
    "  messages=mensagens,\n",
    "  tools=tools,\n",
    "  tool_choice= {'type': 'function', 'function':  {'name': 'obter_temperatura_atual'}}\n",
    ")\n",
    "mensagem = resposta.choices[0].message\n",
    "print('Content: ', mensagem.content)\n",
    "print('Tools: ', mensagem.tool_calls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Content:  None\n",
      "Tools:  [ChatCompletionMessageToolCall(id='call_R5fOPkkx2GNdTeJRMoAqg7RX', function=Function(arguments='{\"local\":\"São Paulo\",\"unidade\":\"celsius\"}', name='obter_temperatura_atual'), type='function')]\n"
     ]
    }
   ],
   "source": [
    "mensagens = [\n",
    "  { 'role': 'user', 'content': 'Olá' }\n",
    "]\n",
    "\n",
    "resposta = client.chat.completions.create(\n",
    "  model='gpt-3.5-turbo-0125',\n",
    "  messages=mensagens,\n",
    "  tools=tools,\n",
    "  tool_choice= {'type': 'function', 'function':  {'name': 'obter_temperatura_atual'}}\n",
    ")\n",
    "mensagem = resposta.choices[0].message\n",
    "print('Content: ', mensagem.content)\n",
    "print('Tools: ', mensagem.tool_calls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
